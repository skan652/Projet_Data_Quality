{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 : Pipeline Hybride - Algorithmes + LLM\n",
    "\n",
    "## Objectif\n",
    "Combiner les **algorithmes classiques** de d√©couverte de FDs avec l'**analyse s√©mantique par LLM** pour identifier les d√©pendances fonctionnelles **significatives**.\n",
    "\n",
    "## Architecture du Pipeline\n",
    "\n",
    "```\n",
    "Dataset ‚Üí [Algorithme FD] ‚Üí FDs candidates ‚Üí [LLM S√©mantique] ‚Üí FDs significatives\n",
    "              ‚Üì                                    ‚Üì\n",
    "         Validation                          √âvaluation\n",
    "         technique                           du sens\n",
    "```\n",
    "\n",
    "## Ce qu'on combine :\n",
    "1. **Task 1** : D√©couverte algorithmique des FDs (pr√©cision technique)\n",
    "2. **Task 2** : Analyse s√©mantique LLM (signification m√©tier)\n",
    "3. **Task 3** : Le√ßons sur l'√©chantillonnage (validation robuste)\n",
    "\n",
    "## R√©sultat attendu\n",
    "Un score hybride pour chaque FD : `Score = Validit√©_technique √ó Pertinence_s√©mantique`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration et imports\n",
    "import anthropic\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "from itertools import combinations\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Configurer Claude\n",
    "CLAUDE_API_KEY = os.getenv(\"CLAUDE_API_KEY\")\n",
    "client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)\n",
    "\n",
    "print(\"‚úÖ Configuration charg√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Charger les datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les datasets\n",
    "datasets = {}\n",
    "\n",
    "# IRIS\n",
    "iris_path = '../Datasets/iris/iris.data'\n",
    "if os.path.exists(iris_path):\n",
    "    datasets['iris'] = pd.read_csv(iris_path, header=None, \n",
    "                                    names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class'])\n",
    "    print(f\"‚úÖ iris: {datasets['iris'].shape}\")\n",
    "\n",
    "# BRIDGES\n",
    "bridges_path = '../Datasets/pittsburgh+bridges/bridges.data.version1'\n",
    "if os.path.exists(bridges_path):\n",
    "    datasets['bridges'] = pd.read_csv(bridges_path, header=None,\n",
    "                                       names=['IDENTIF', 'RIVER', 'LOCATION', 'ERECTED', 'PURPOSE', \n",
    "                                              'LENGTH', 'LANES', 'CLEAR-G', 'T-OR-D', 'MATERIAL',\n",
    "                                              'SPAN', 'REL-L', 'TYPE'])\n",
    "    print(f\"‚úÖ bridges: {datasets['bridges'].shape}\")\n",
    "\n",
    "# NURSERY\n",
    "nursery_path = '../Datasets/nursery/nursery.data'\n",
    "if os.path.exists(nursery_path):\n",
    "    datasets['nursery'] = pd.read_csv(nursery_path, header=None,\n",
    "                                       names=['parents', 'has_nurs', 'form', 'children', \n",
    "                                              'housing', 'finance', 'social', 'health', 'class'])\n",
    "    print(f\"‚úÖ nursery: {datasets['nursery'].shape}\")\n",
    "\n",
    "print(f\"\\nüìä Datasets charg√©s: {list(datasets.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithme de d√©couverte de FDs\n",
    "\n",
    "On impl√©mente un algorithme simplifi√© qui :\n",
    "1. Teste toutes les paires (LHS, RHS) possibles\n",
    "2. Calcule le taux de validit√© de chaque FD\n",
    "3. Retourne les FDs avec un taux > seuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def check_fd(df, lhs_cols, rhs_col):\n    \"\"\"\n    V√©rifie si une FD tient dans un DataFrame.\n    Retourne: (holds: bool, validity_rate: float, violations: int)\n    \"\"\"\n    if isinstance(lhs_cols, str):\n        lhs_cols = [lhs_cols]\n    \n    # Grouper par LHS et compter les valeurs uniques de RHS\n    grouped = df.groupby(list(lhs_cols))[rhs_col].nunique()\n    \n    # Une FD tient si chaque groupe a exactement 1 valeur pour RHS\n    violations = (grouped > 1).sum()\n    total_groups = len(grouped)\n    \n    if total_groups == 0:\n        return False, 0, 0\n    \n    validity_rate = ((total_groups - violations) / total_groups) * 100\n    holds = violations == 0\n    \n    return holds, validity_rate, violations\n\n\ndef discover_fds(df, min_validity=100, max_lhs_size=2):\n    \"\"\"\n    D√©couvre les FDs dans un DataFrame.\n    \n    Args:\n        df: DataFrame √† analyser\n        min_validity: Taux minimum de validit√© (100 = FD exacte)\n        max_lhs_size: Taille maximale du LHS\n    \n    Returns:\n        Liste de FDs avec leurs statistiques\n    \"\"\"\n    columns = list(df.columns)\n    fds = []\n    \n    # Tester toutes les combinaisons de LHS\n    for lhs_size in range(1, max_lhs_size + 1):\n        for lhs_cols in combinations(columns, lhs_size):\n            lhs_cols = list(lhs_cols)\n            \n            # Tester chaque RHS possible\n            for rhs_col in columns:\n                if rhs_col in lhs_cols:\n                    continue  # RHS ne peut pas √™tre dans LHS\n                \n                holds, validity, violations = check_fd(df, lhs_cols, rhs_col)\n                \n                if validity >= min_validity:\n                    fds.append({\n                        'lhs': lhs_cols,\n                        'rhs': rhs_col,\n                        'fd_string': f\"{', '.join(lhs_cols)} -> {rhs_col}\",\n                        'holds': holds,\n                        'validity': validity,\n                        'violations': violations,\n                        'lhs_size': len(lhs_cols)\n                    })\n    \n    return fds\n\nprint(\"‚úÖ Fonctions check_fd() et discover_fds() d√©finies\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. D√©couvrir les FDs candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# D√©couvrir les FDs pour chaque dataset\n# On cherche les FDs APPROXIMATIVES (>= 90% de validit√©) pour avoir plus de r√©sultats int√©ressants\n\nall_discovered_fds = {}\n\nprint(\"=\"*80)\nprint(\"D√âCOUVERTE ALGORITHMIQUE DES FDs (exactes ET approximatives)\")\nprint(\"=\"*80)\n\nfor name, df in datasets.items():\n    print(f\"\\nüîç Analyse de {name}...\")\n    \n    # D√©couvrir les FDs avec au moins 90% de validit√©\n    fds = discover_fds(df, min_validity=90, max_lhs_size=2)\n    \n    # Trier par validit√© d√©croissante\n    fds = sorted(fds, key=lambda x: (-x['validity'], x['lhs_size']))\n    \n    # Garder les 15 plus int√©ressantes\n    fds_filtered = fds[:15]\n    \n    all_discovered_fds[name] = fds_filtered\n    \n    print(f\"   ‚úÖ {len(fds)} FDs trouv√©es (>= 90% validit√©)\")\n    print(f\"   üìã Top FDs:\")\n    for fd in fds_filtered[:5]:\n        status = \"‚úì 100%\" if fd['validity'] == 100 else f\"~{fd['validity']:.0f}%\"\n        print(f\"      - {fd['fd_string']} [{status}]\")\n\nprint(f\"\\nüìä Total: {sum(len(fds) for fds in all_discovered_fds.values())} FDs candidates\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse s√©mantique par LLM\n",
    "\n",
    "Le LLM √©value chaque FD sur plusieurs crit√®res :\n",
    "- **Signification** : La FD a-t-elle un sens m√©tier ?\n",
    "- **Utilit√©** : Est-elle utile pour comprendre les donn√©es ?\n",
    "- **Type** : Cl√© primaire, r√®gle m√©tier, accidentelle, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fd_semantically(fd_string, dataset_name, columns_info):\n",
    "    \"\"\"\n",
    "    Utilise le LLM pour √©valuer s√©mantiquement une FD.\n",
    "    Retourne un score de 0 √† 10 et une cat√©gorie.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Tu es un expert en bases de donn√©es et en qualit√© des donn√©es.\n",
    "\n",
    "Dataset: {dataset_name}\n",
    "Colonnes disponibles: {columns_info}\n",
    "\n",
    "√âvalue cette d√©pendance fonctionnelle: {fd_string}\n",
    "\n",
    "R√©ponds UNIQUEMENT avec ce format JSON (pas d'autre texte):\n",
    "{{\n",
    "    \"score\": <nombre de 0 √† 10>,\n",
    "    \"category\": \"<une parmi: key, business_rule, derived, accidental, meaningless>\",\n",
    "    \"reason\": \"<explication courte en 10 mots max>\"\n",
    "}}\n",
    "\n",
    "Crit√®res:\n",
    "- score 8-10: FD significative (r√®gle m√©tier, cl√© naturelle)\n",
    "- score 5-7: FD utile mais pas fondamentale\n",
    "- score 2-4: FD technique ou d√©riv√©e\n",
    "- score 0-1: FD accidentelle ou sans sens\n",
    "\n",
    "Categories:\n",
    "- key: identifiant unique ou cl√© primaire\n",
    "- business_rule: r√®gle m√©tier logique\n",
    "- derived: attribut calcul√©/d√©riv√©\n",
    "- accidental: corr√©lation sans causalit√©\n",
    "- meaningless: pas de sens s√©mantique\"\"\"\n",
    "    \n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=200,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        response_text = message.content[0].text.strip()\n",
    "        \n",
    "        # Parser le JSON\n",
    "        # Nettoyer si n√©cessaire\n",
    "        if response_text.startswith(\"```\"):\n",
    "            response_text = response_text.split(\"```\")[1]\n",
    "            if response_text.startswith(\"json\"):\n",
    "                response_text = response_text[4:]\n",
    "        \n",
    "        result = json.loads(response_text)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"score\": 5, \"category\": \"unknown\", \"reason\": f\"Erreur: {str(e)[:30]}\"}\n",
    "\n",
    "print(\"‚úÖ Fonction d'√©valuation s√©mantique d√©finie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pipeline Hybride Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex√©cuter le pipeline hybride sur toutes les FDs d√©couvertes\n",
    "\n",
    "hybrid_results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PIPELINE HYBRIDE : Algorithme + LLM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset_name, fds in all_discovered_fds.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dataset: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    columns_info = list(datasets[dataset_name].columns)\n",
    "    \n",
    "    for fd in fds[:10]:  # Limiter √† 10 FDs par dataset\n",
    "        print(f\"\\nüîÑ √âvaluation: {fd['fd_string']}\")\n",
    "        \n",
    "        # √âvaluation s√©mantique par LLM\n",
    "        semantic_eval = evaluate_fd_semantically(fd['fd_string'], dataset_name, columns_info)\n",
    "        \n",
    "        # Calculer le score hybride\n",
    "        technical_score = fd['validity'] / 10  # 0-10\n",
    "        semantic_score = semantic_eval.get('score', 5)\n",
    "        hybrid_score = (technical_score + semantic_score) / 2\n",
    "        \n",
    "        result = {\n",
    "            'dataset': dataset_name,\n",
    "            'fd': fd['fd_string'],\n",
    "            'technical_validity': fd['validity'],\n",
    "            'semantic_score': semantic_score,\n",
    "            'hybrid_score': hybrid_score,\n",
    "            'category': semantic_eval.get('category', 'unknown'),\n",
    "            'reason': semantic_eval.get('reason', '')\n",
    "        }\n",
    "        hybrid_results.append(result)\n",
    "        \n",
    "        # Afficher le r√©sultat\n",
    "        print(f\"   üìä Technique: {fd['validity']:.0f}% | S√©mantique: {semantic_score}/10 | Hybride: {hybrid_score:.1f}\")\n",
    "        print(f\"   üè∑Ô∏è Cat√©gorie: {semantic_eval.get('category', '?')}\")\n",
    "        print(f\"   üí¨ Raison: {semantic_eval.get('reason', '?')}\")\n",
    "        \n",
    "        time.sleep(1)  # Pause pour l'API\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline hybride termin√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tableau des r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le tableau des r√©sultats\n",
    "results_df = pd.DataFrame(hybrid_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABLEAU DES R√âSULTATS HYBRIDES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Trier par score hybride d√©croissant\n",
    "results_df = results_df.sort_values('hybrid_score', ascending=False)\n",
    "\n",
    "display(results_df[['dataset', 'fd', 'technical_validity', 'semantic_score', 'hybrid_score', 'category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classification des FDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier les FDs par cat√©gorie\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION DES FDs PAR CAT√âGORIE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# FDs significatives (score hybride >= 7)\n",
    "significant = results_df[results_df['hybrid_score'] >= 7]\n",
    "print(f\"\\nüåü FDs SIGNIFICATIVES (score >= 7): {len(significant)}\")\n",
    "for _, row in significant.iterrows():\n",
    "    print(f\"   [{row['dataset']}] {row['fd']} (score: {row['hybrid_score']:.1f}, {row['category']})\")\n",
    "\n",
    "# FDs utiles (score entre 5 et 7)\n",
    "useful = results_df[(results_df['hybrid_score'] >= 5) & (results_df['hybrid_score'] < 7)]\n",
    "print(f\"\\nüëç FDs UTILES (5 <= score < 7): {len(useful)}\")\n",
    "for _, row in useful.iterrows():\n",
    "    print(f\"   [{row['dataset']}] {row['fd']} (score: {row['hybrid_score']:.1f}, {row['category']})\")\n",
    "\n",
    "# FDs √† ignorer (score < 5)\n",
    "ignore = results_df[results_df['hybrid_score'] < 5]\n",
    "print(f\"\\n‚ùå FDs √Ä IGNORER (score < 5): {len(ignore)}\")\n",
    "for _, row in ignore.iterrows():\n",
    "    print(f\"   [{row['dataset']}] {row['fd']} (score: {row['hybrid_score']:.1f}, {row['category']})\")\n",
    "\n",
    "# Statistiques par cat√©gorie\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTIQUES PAR CAT√âGORIE\")\n",
    "print(\"=\"*80)\n",
    "category_stats = results_df.groupby('category').agg({\n",
    "    'fd': 'count',\n",
    "    'hybrid_score': 'mean'\n",
    "}).rename(columns={'fd': 'count', 'hybrid_score': 'avg_score'})\n",
    "display(category_stats.sort_values('avg_score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Distribution des scores hybrides\n",
    "axes[0].hist(results_df['hybrid_score'], bins=10, edgecolor='black', color='steelblue')\n",
    "axes[0].set_xlabel('Score Hybride')\n",
    "axes[0].set_ylabel('Nombre de FDs')\n",
    "axes[0].set_title('Distribution des Scores Hybrides')\n",
    "axes[0].axvline(x=7, color='green', linestyle='--', label='Seuil significatif')\n",
    "axes[0].axvline(x=5, color='orange', linestyle='--', label='Seuil utile')\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. Score technique vs s√©mantique\n",
    "colors = {'key': 'green', 'business_rule': 'blue', 'derived': 'orange', \n",
    "          'accidental': 'red', 'meaningless': 'gray', 'unknown': 'purple'}\n",
    "for cat in results_df['category'].unique():\n",
    "    subset = results_df[results_df['category'] == cat]\n",
    "    axes[1].scatter(subset['technical_validity'], subset['semantic_score'], \n",
    "                    label=cat, c=colors.get(cat, 'black'), s=100, alpha=0.7)\n",
    "axes[1].set_xlabel('Validit√© Technique (%)')\n",
    "axes[1].set_ylabel('Score S√©mantique (0-10)')\n",
    "axes[1].set_title('Technique vs S√©mantique')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. R√©partition par cat√©gorie\n",
    "category_counts = results_df['category'].value_counts()\n",
    "axes[2].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%',\n",
    "            colors=[colors.get(c, 'gray') for c in category_counts.index])\n",
    "axes[2].set_title('R√©partition par Cat√©gorie')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/task4_hybrid_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualisation sauvegard√©e: results/task4_hybrid_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sauvegarder les r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les r√©sultats\n",
    "results_df.to_csv('../results/task4_hybrid_results.csv', index=False)\n",
    "print(\"‚úÖ R√©sultats sauvegard√©s: results/task4_hybrid_results.csv\")\n",
    "\n",
    "# Sauvegarder en JSON aussi\n",
    "with open('../results/task4_hybrid_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(hybrid_results, f, indent=2, ensure_ascii=False)\n",
    "print(\"‚úÖ R√©sultats JSON: results/task4_hybrid_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion et Synth√®se\n",
    "\n",
    "### Avantages du Pipeline Hybride\n",
    "\n",
    "| Approche | Forces | Faiblesses |\n",
    "|----------|--------|------------|\n",
    "| **Algorithme seul** | Pr√©cis, exhaustif | Trouve des FDs sans sens |\n",
    "| **LLM seul** | Comprend le contexte | Peut halluciner des FDs |\n",
    "| **Hybride** | Pr√©cis ET significatif | Meilleur des deux mondes |\n",
    "\n",
    "### Ce qu'on a appris\n",
    "\n",
    "1. **Les algorithmes trouvent TOUTES les FDs** valides techniquement, mais beaucoup sont triviales ou accidentelles\n",
    "\n",
    "2. **Le LLM filtre par le sens** : il distingue les cl√©s primaires, les r√®gles m√©tier, et les corr√©lations accidentelles\n",
    "\n",
    "3. **Le score hybride** combine les deux pour identifier les FDs vraiment utiles\n",
    "\n",
    "### Recommandation finale\n",
    "\n",
    "> **Utilisez toujours une approche hybride** : les algorithmes pour la pr√©cision, les LLMs pour la pertinence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© final\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"R√âSUM√â FINAL - TASK 4 : PIPELINE HYBRIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Statistiques globales:\")\n",
    "print(f\"   - Datasets analys√©s: {len(all_discovered_fds)}\")\n",
    "print(f\"   - FDs √©valu√©es: {len(results_df)}\")\n",
    "print(f\"   - FDs significatives (score >= 7): {len(results_df[results_df['hybrid_score'] >= 7])}\")\n",
    "print(f\"   - FDs utiles (5-7): {len(results_df[(results_df['hybrid_score'] >= 5) & (results_df['hybrid_score'] < 7)])}\")\n",
    "print(f\"   - FDs √† ignorer (< 5): {len(results_df[results_df['hybrid_score'] < 5])}\")\n",
    "\n",
    "print(f\"\\nüìà Score hybride moyen par cat√©gorie:\")\n",
    "for cat, score in results_df.groupby('category')['hybrid_score'].mean().sort_values(ascending=False).items():\n",
    "    print(f\"   - {cat}: {score:.1f}\")\n",
    "\n",
    "print(f\"\\nüìÅ Fichiers g√©n√©r√©s:\")\n",
    "print(f\"   - results/task4_hybrid_results.csv\")\n",
    "print(f\"   - results/task4_hybrid_results.json\")\n",
    "print(f\"   - results/task4_hybrid_analysis.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PIPELINE HYBRIDE TERMIN√â AVEC SUCC√àS !\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}